# config.toml - ENHANCED MULTI-PROVIDER LLM CONFIGURATION
# Centralized configuration with multi-provider support and backward compatibility

# =============================================================================
# MULTI-PROVIDER LLM CONFIGURATION
# =============================================================================

[llm]
    # GLOBAL LLM SETTINGS
    primary_provider = "gemini"           # Primary LLM provider to use
    fallback_provider = "openai"          # Fallback if primary fails
    enable_fallback = true                # Enable/disable fallback mechanism
    retry_attempts = 2                    # Number of retry attempts per provider
    timeout_seconds = 60                  # Default timeout for LLM calls

    # BACKWARD COMPATIBILITY - These will be used if provider-specific config not found
    model = "gemini-1.5-flash-latest"
    api_key = ""                          # Will be overridden by environment variables
    base_url = "https://generativelanguage.googleapis.com/v1beta/models"
    max_tokens = 2000
    temperature = 0.1

    # PROVIDER-SPECIFIC CONFIGURATIONS
    [llm.providers]

        [llm.providers.gemini]
            enabled = true
            model = "gemini-1.5-flash-latest"
            api_key = ""                  # Will use GOOGLE_API_KEY or GEMINI_API_KEY from .env
            base_url = "https://generativelanguage.googleapis.com/v1beta/models"
            max_tokens = 4000
            temperature = 0.1
            timeout = 60
            # Gemini-specific parameters
            safety_settings = "default"
            generation_config = {}

        [llm.providers.openai]
            enabled = true
            model = "gpt-4"
            api_key = ""                  # Will use OPENAI_API_KEY from .env
            base_url = "https://api.openai.com/v1/chat/completions"
            max_tokens = 4000
            temperature = 0.1
            timeout = 60
            # OpenAI-specific parameters
            frequency_penalty = 0.0
            presence_penalty = 0.0
            top_p = 1.0

        [llm.providers.anthropic]
            enabled = true
            model = "claude-3-5-sonnet-20241022"
            api_key = ""                  # Will use ANTHROPIC_API_KEY from .env
            base_url = "https://api.anthropic.com/v1/messages"
            max_tokens = 4000
            temperature = 0.1
            timeout = 60
            # Anthropic-specific parameters
            anthropic_version = "2023-06-01"

        [llm.providers.mistral]
            enabled = true
            model = "mistral-large-latest"
            api_key = ""                  # Will use MISTRAL_API_KEY from .env
            base_url = "https://api.mistral.ai/v1/chat/completions"
            max_tokens = 4000
            temperature = 0.1
            timeout = 60
            # Mistral-specific parameters
            safe_prompt = false

        [llm.providers.ollama]
            enabled = false               # Disabled by default (local setup required)
            model = "llama2"
            api_key = "local"            # Not needed for local Ollama
            base_url = "http://localhost:11434/v1/chat/completions"
            max_tokens = 4000
            temperature = 0.1
            timeout = 120                # Longer timeout for local models
            # Ollama-specific parameters
            num_predict = 4000
            top_k = 40
            top_p = 0.9

    # LLM OCR CONFIGURATION (Enhanced for multi-provider)
    [llm.ocr]
        primary_method = "gemini"         # Primary OCR method
        fallback_enabled = true           # Enable OCR fallback
        confidence_threshold = 0.7        # Minimum confidence threshold
        timeout_seconds = 60              # OCR-specific timeout
        max_retries = 2                   # OCR-specific retries

        # Provider-specific OCR configurations
        [llm.ocr.providers]

            [llm.ocr.providers.gemini]
                enabled = true
                model = "gemini-1.5-flash-latest"
                api_key = ""              # Will use GOOGLE_API_KEY from .env
                max_tokens = 4000
                temperature = 0.1

            [llm.ocr.providers.openai]
                enabled = true
                model = "gpt-4o"
                api_key = ""              # Will use OPENAI_API_KEY from .env
                max_tokens = 4000
                temperature = 0.1

            [llm.ocr.providers.claude]
                enabled = true
                model = "claude-3-5-sonnet-20241022"
                api_key = ""              # Will use ANTHROPIC_API_KEY from .env
                max_tokens = 4000
                temperature = 0.1

            [llm.ocr.providers.mistral]
                enabled = true
                model = "pixtral-12b-2409"
                api_key = ""              # Will use MISTRAL_API_KEY from .env
                max_tokens = 4000
                temperature = 0.1

# =============================================================================
# SPECIFIC LLM TASK CONFIGURATIONS
# =============================================================================

[triple_extraction]
    # Can use different provider for triple extraction
    provider = "gemini"                   # Provider to use (falls back to primary if not specified)
    model = "gemini-1.5-flash-latest"    # Model override (optional)
    api_key = ""                          # Will inherit from provider config
    base_url = ""                         # Will inherit from provider config
    max_tokens = 2000
    temperature = 0.1
    # Task-specific parameters
    extraction_mode = "comprehensive"
    include_confidence = true

[relationship_inference]
    provider = "gemini"                   # Provider for relationship inference
    enabled = true
    model = "gemini-1.5-flash-latest"
    api_key = ""
    base_url = ""
    max_tokens = 1500
    temperature = 0.1
    # Task-specific parameters
    inference_threshold = 0.7
    max_relationships_per_call = 10

[within_community_inference]
    provider = "gemini"                   # Provider for community inference
    enabled = true
    model = "gemini-1.5-flash-latest"
    api_key = ""
    base_url = ""
    max_tokens = 1000
    temperature = 0.1
    # Task-specific parameters
    community_size_threshold = 5
    max_inferences_per_community = 3

[text_sanitization]
    provider = "gemini"                   # Provider for text sanitization
    enabled = true
    model = "gemini-1.5-flash-latest"
    max_tokens = 4000
    temperature = 0.1
    # Sanitization-specific parameters
    sanitization_level = "moderate"
    preserve_structure = true
    remove_personal_info = false

[cypher_correction]
    provider = "gemini"                   # Provider for Cypher query correction
    enabled = true
    model = "gemini-1.5-flash-latest"
    max_tokens = 1000
    temperature = 0.0                     # Very low temperature for precise corrections
    # Correction-specific parameters
    max_correction_attempts = 3
    validate_syntax = true

# =============================================================================
# PROVIDER USAGE POLICIES
# =============================================================================

[provider_policies]
    # Cost management
    enable_cost_tracking = true
    daily_cost_limit = 50.0              # USD
    monthly_cost_limit = 1000.0          # USD

    # Usage priorities by task type
    [provider_policies.task_priorities]
        ocr = ["gemini", "openai", "claude", "mistral"]
        triple_extraction = ["gemini", "openai", "mistral", "claude"]
        inference = ["gemini", "openai", "claude", "mistral"]
        correction = ["gemini", "openai", "claude", "mistral"]

    # Fallback chains
    [provider_policies.fallback_chains]
        gemini = ["openai", "claude"]
        openai = ["gemini", "claude"]
        claude = ["gemini", "openai"]
        mistral = ["gemini", "openai"]

# =============================================================================
# EXISTING CONFIGURATIONS (Maintained for backward compatibility)
# =============================================================================

[inference]
    enabled = true
    llm_max_community_pairs = 5
    llm_communities_to_consider = 3
    llm_within_communities_to_process = 2
    llm_max_pairs_per_community = 5

[neo4j]
    uri = "bolt://neo4j:7687"
    user = "neo4j"
    password = ""
    database = "neo4j"

[database]
    # PostgreSQL connection settings
    host = "postgres"
    port = 5432
    database = "graphrag_chat"
    user = "postgres"
    password = ""
    min_pool_size = 2
    max_pool_size = 10

    # Alternative: Use DATABASE_URL (overrides individual settings)
    # url = "postgresql://postgres:your_password@localhost:5432/graphrag_chat"

[chat]
    # Chat system settings
    max_conversations_per_user = 100
    max_messages_per_conversation = 1000
    auto_title_generation = true
    enable_streaming = true
    enable_search = true
    search_results_limit = 50
    message_feedback_enabled = true
    default_user_id = "default_user"

[users]
    default_username = "default_user"
    require_authentication = false

[vector_db]
    persist_directory = "./chroma_db_pipeline"
    collection_name = "doc_pipeline_embeddings"

[embeddings]
    model_name = "all-MiniLM-L6-v2"

[chunking]
    chunk_size = 1000
    overlap = 100

[standardization]
    enabled = true
    debug_logging = false
    max_merge_attempts = 100
    similarity_threshold = 0.8
    cross_type_merging = true
    log_rejected_merges = false
    log_level = "WARNING"

    [standardization.synonyms]
    # ACTIVELY USED by entity_standardization.py for synonym mapping
    "wo" = "Work Order"
    "work order" = "Work Order"
    "work-order" = "Work Order"
    "maintenance order" = "Work Order"
    "service order" = "Work Order"
    "repair order" = "Work Order"

    # EQUIPMENT STANDARDIZATION
    "compressor" = "Compressor Unit"
    "pump" = "Pump Unit"
    "motor" = "Motor Unit"
    "generator" = "Generator Unit"
    "turbine" = "Turbine Unit"
    "valve" = "Valve Unit"
    "tank" = "Storage Tank"
    "vessel" = "Pressure Vessel"

    # FACILITY & LOCATION TERMS
    "bldg" = "Building"
    "bldg." = "Building"
    "building" = "Building"
    "facility" = "Facility"
    "plant" = "Plant"
    "site" = "Site"
    "location" = "Location"
    "area" = "Area"
    "zone" = "Zone"
    "section" = "Section"
    "unit" = "Unit"

    # MAINTENANCE TYPES
    "pm" = "Preventive Maintenance"
    "preventive maintenance" = "Preventive Maintenance"
    "preventative maintenance" = "Preventive Maintenance"
    "corrective maintenance" = "Corrective Maintenance"
    "emergency repair" = "Emergency Maintenance"
    "breakdown maintenance" = "Emergency Maintenance"
    "scheduled maintenance" = "Preventive Maintenance"
    "routine maintenance" = "Preventive Maintenance"

    # TECHNICAL TERMS
    "hp" = "Horsepower"
    "horsepower" = "Horsepower"
    "rpm" = "RPM"
    "psi" = "PSI"
    "gpm" = "GPM"
    "cfm" = "CFM"
    "kwh" = "kWh"
    "kilowatt hour" = "kWh"
    "kilowatt-hour" = "kWh"

    # STATUS TERMS
    "complete" = "Complete"
    "completed" = "Complete"
    "finished" = "Complete"
    "done" = "Complete"
    "in progress" = "In Progress"
    "in-progress" = "In Progress"
    "pending" = "Pending"
    "open" = "Open"
    "closed" = "Closed"
    "cancelled" = "Cancelled"
    "canceled" = "Cancelled"

    # PARTS & MATERIALS
    "bearing" = "Bearing"
    "seal" = "Seal"
    "gasket" = "Gasket"
    "o-ring" = "O-Ring"
    "o ring" = "O-Ring"
    "filter" = "Filter"
    "belt" = "Belt"
    "coupling" = "Coupling"
    "impeller" = "Impeller"
    "rotor" = "Rotor"
    "stator" = "Stator"

    # PERSONNEL TITLES
    "tech" = "Technician"
    "technician" = "Technician"
    "mechanic" = "Technician"
    "engineer" = "Engineer"
    "supervisor" = "Supervisor"
    "foreman" = "Supervisor"
    "manager" = "Manager"
    "operator" = "Operator"

    # INVOICE & FINANCIAL TERMS
    "inv" = "Invoice"
    "invoice" = "Invoice"
    "bill" = "Invoice"
    "receipt" = "Receipt"
    "po" = "Purchase Order"
    "purchase order" = "Purchase Order"
    "p.o." = "Purchase Order"
    "afe" = "AFE Number"
    "afe number" = "AFE Number"
    "cost center" = "Cost Center"
    "gl code" = "GL Code"
    "account code" = "GL Code"

    # DOCUMENT TYPES
    "report" = "Report"
    "checklist" = "Checklist"
    "inspection" = "Inspection Report"
    "test report" = "Test Report"
    "certificate" = "Certificate"
    "permit" = "Permit"
    "procedure" = "Procedure"
    "manual" = "Manual"

    # TIME & SCHEDULING
    "hrs" = "Hours"
    "hours" = "Hours"
    "hr" = "Hours"
    "mins" = "Minutes"
    "minutes" = "Minutes"
    "min" = "Minutes"
    "shift" = "Shift"
    "day shift" = "Day Shift"
    "night shift" = "Night Shift"
    "weekend" = "Weekend"
    "overtime" = "Overtime"

    # SAFETY & COMPLIANCE
    "safety" = "Safety"
    "hazard" = "Hazard"
    "risk" = "Risk"
    "lockout" = "Lockout/Tagout"
    "loto" = "Lockout/Tagout"
    "permit to work" = "Work Permit"
    "work permit" = "Work Permit"
    "confined space" = "Confined Space"
    "hot work" = "Hot Work"

    # MEASUREMENTS & UNITS
    "ft" = "Feet"
    "feet" = "Feet"
    "in" = "Inches"
    "inches" = "Inches"
    "mm" = "Millimeters"
    "cm" = "Centimeters"
    "m" = "Meters"
    "kg" = "Kilograms"
    "lbs" = "Pounds"
    "pounds" = "Pounds"
    "tons" = "Tons"
    "gallons" = "Gallons"
    "gal" = "Gallons"
    "liters" = "Liters"
    "l" = "Liters"

[nlp]
    coreference_resolution_enabled = false
    spacy_model_name = "en_core_web_trf"

[performance]
    max_workers = 2
    batch_size = 1
    memory_limit_mb = 4096
    enable_caching = true
    cache_ttl_hours = 24

[logging]
    level = "INFO"
    mask_api_keys = true
    mask_passwords = true
    ocr_level = "INFO"
    entity_standardization_level = "WARNING"
    neo4j_level = "INFO"

    [logging.loggers]
    "src.knowledge_graph.entity_standardization" = "WARNING"
    "src.utils.ocr_storage" = "INFO"
    "llm" = "INFO"                        # Logging level for LLM operations
    "llm.providers" = "DEBUG"             # Detailed provider logging

# BACKWARD COMPATIBILITY SECTIONS (Maintained as-is)
[mistral]
    api_key = ""
    model = "pixtral-12b-2409"
    timeout = 30

[caching]
    enabled = true
    cache_ttl_hours = 24

[pipeline]
    standardization_enabled = true
    inference_enabled = true
    cache_enabled = true
    text_sanitization_enabled = true
    text_sanitization_mode = "strict"
    max_file_size_mb = 50
    max_processing_time_minutes = 30
    concurrent_file_limit = 1

[metadata]
    extract_entities = true
    classify_documents = true
    analyze_quality = true
    create_chunks = true
    chunk_size = 1000
    extract_companies = true
    extract_people = true
    extract_locations = true
    extract_dates = true
    extract_money = true
    extract_equipment = true
    enable_document_type_detection = true
    enable_category_classification = true
    classification_confidence_threshold = 0.5
    enable_readability_scoring = true
    enable_complexity_analysis = true
    enable_issue_detection = true

[file_processing]
    supported_types = ["application/pdf", "image/png", "image/jpeg", "image/jpg", "text/plain", "text/csv", "text/html", "text/xml"]
    pdf_dpi = 200
    pdf_max_pages = 10
    enhance_images = true
    image_contrast_boost = 1.2
    image_sharpness_boost = 1.1
    handle_text_files_directly = true
    text_encoding_fallback = ["utf-8", "latin-1", "cp1252", "ascii"]

[storage]
    enable_ocr_storage = true
    storage_directory = "./ocr_outputs"
    save_metadata = true
    save_chunks = true
    enable_csv_export = true
    enable_excel_export = true
    enable_json_export = true
    enable_zip_export = true

[universal]
    enable_universal_patterns = true
    manual_industry = ""
    auto_detect_industry = true
    confidence_threshold = 0.6
    adapt_to_schema = true
    use_domain_keywords = true
    learn_from_data = true

[query_engine]
    enable_advanced_cypher = true
    pattern_confidence_threshold = 0.6
    hybrid_mode = true
    entity_linking_fuzzy_threshold = 70
    enable_query_caching = true
    max_cypher_retries = 2

[pattern_library]
    use_universal_patterns = true
    fallback_to_base_patterns = true
    pattern_confidence_threshold = 0.5
    max_patterns_per_category = 3

[analytics]
    track_generation_stats = true
    store_successful_patterns = true
    enable_few_shot_learning = true